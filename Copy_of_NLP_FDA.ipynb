{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPmfYSmU7M76N7W6SYwZfvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/NLP_Public/blob/main/Copy_of_NLP_FDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Introduction\n",
        "\n",
        "In this notebook, we aim to harness a **Natural Language Model (NLM)** to query and analyze data sourced from the **FDA (Food and Drug Administration)**. By structuring information in a context–question–answer format, we can leverage advanced language processing techniques to extract insights from clinical and regulatory documents.\n",
        "\n",
        "## Key Goals\n",
        "\n",
        "1. **Data Preparation**: Organize FDA dataset content into a question-answer-friendly format.\n",
        "2. **Model Integration**: Use a language model capable of understanding domain-specific terminology and context.\n",
        "3. **Query Execution**: Demonstrate how to pose queries about FDA data (e.g., clinical trials, drug efficacy, labeling information) and retrieve precise answers.\n",
        "4. **Validation**: Ensure that generated responses align with the source information to maintain accuracy and reliability.\n",
        "\n",
        "Through this project, we seek to streamline how we interact with complex FDA data, making it more accessible and actionable using the power of natural language processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "55MMSE2kD0Nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Weights & Biases (W&B)\n",
        "\n",
        "**Weights & Biases** (often abbreviated as **W&B**) is a powerful experiment tracking and model management tool commonly used in machine learning and deep learning projects. It helps you monitor and compare different runs or experiments by logging various metrics, parameters, and artifacts.\n",
        "\n",
        "### Why Use `wandb` (W&B)?\n",
        "\n",
        "1. **Experiment Tracking**  \n",
        "   W&B keeps a detailed record of your training metrics (e.g., loss, accuracy) over time. It allows you to visualize these metrics in real time and compare them across different runs.\n",
        "\n",
        "2. **Hyperparameter Management**  \n",
        "   You can log hyperparameters such as learning rate, batch size, or model architecture details. This makes it straightforward to see which parameter settings lead to better performance.\n",
        "\n",
        "3. **Collaboration and Reproducibility**  \n",
        "   Using W&B, you can easily share your results and experiments with your team. This shared environment helps maintain reproducibility since everything you need to recreate a run (code, hyperparameters, metrics, etc.) is logged.\n",
        "\n",
        "4. **Artifact Storage**  \n",
        "   W&B can store model checkpoints, plots, and other generated files. You can later retrieve them for analysis, comparison, or deployment.\n",
        "\n",
        "### How We’re Using W&B\n",
        "\n",
        "In this project, we leverage W&B to:\n",
        "- **Track** the performance of our model(s) over multiple training runs.  \n",
        "- **Compare** the effects of different hyperparameter settings or data pre-processing strategies.  \n",
        "- **Log** important artifacts like final model weights, figures, and tables for easy reference.\n",
        "\n",
        "By using W&B, we aim to streamline the experimentation workflow, making it more transparent and easier to improve and reproduce.\n"
      ],
      "metadata": {
        "id": "Dv0D2YZmEfon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "import random"
      ],
      "metadata": {
        "id": "GN_hd7Arrt8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Set Up Colab Environment\n",
        "Open a new Colab notebook and make sure you enable GPU acceleration. We'll be using the A100, which is a powerful GPU and should be able to handle the rifors of NLP training"
      ],
      "metadata": {
        "id": "3VY5qEHwn1uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install & Imports\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import random\n",
        "import torch\n",
        "# ... other imports\n",
        "\n",
        "# 2. Initialize W&B\n",
        "wandb.init(\n",
        "    project=\"nlp_fda\",\n",
        "    config={\n",
        "        \"learning_rate\": 0.02,\n",
        "        \"architecture\": \"CNN\",\n",
        "        \"dataset\": \"CIFAR-100\",\n",
        "        \"epochs\": 10,\n",
        "    }\n",
        ")\n",
        "\n",
        "# 3. Training / Loop\n",
        "epochs = 10\n",
        "offset = random.random() / 5\n",
        "for epoch in range(2, epochs):\n",
        "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "\n",
        "    # log metrics to wandb\n",
        "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
        "\n",
        "# 4. Finish the run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "eo3zCxEDtrHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPMIsehWmOWJ"
      },
      "outputs": [],
      "source": [
        "# In Colab: Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "\n",
        "!nvidia-smi  # Optional: Check that GPU is enabled\n",
        "!pip install transformers datasets  # Hugging Face libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. (Optional) Fetching FDA Data\n",
        "We can fetch some sample data from the FDA drug label endpoint. For demonstration, we’ll just pull one or two records and store relevant text. You’ll want more data for a real-world model."
      ],
      "metadata": {
        "id": "YoNKd7zVoQb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "base_url = \"https://api.fda.gov/drug/label.json\"\n",
        "params = {\n",
        "    \"search\": \"brand_name:advil\",  # Example query\n",
        "    \"limit\": 1\n",
        "}\n",
        "\n",
        "# The FDA drug label endpoint does NOT require an API key.\n",
        "# We can simply make a GET request with the desired query parameters.\n",
        "response = requests.get(base_url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Extract the relevant text fields from the response as needed.\n",
        "if \"results\" in data:\n",
        "    record = data[\"results\"][0]\n",
        "    warnings_text = record.get(\"warnings\", [\"\"])[0]\n",
        "else:\n",
        "    warnings_text = \"No data found.\"\n",
        "\n",
        "print(warnings_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "tJjR5dZOmfWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a Small QA Dataset\n",
        "We’ll simulate a Question-Answer dataset. Each sample will have:\n",
        "\n",
        "* `context`: The text from the FDA label (e.g., warnings/adverse reactions).\n",
        "* `question`: What we want to extract (e.g., “What are the warnings?”).\n",
        "* `answers`: The labeled correct answer (including its start and end character indices in `context`).\n",
        "\n",
        "For demonstration, we’ll do minimal manual labeling here. In a real project, I’d systematically label many examples (or build them with a script).\n"
      ],
      "metadata": {
        "id": "KNBCa61XonKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Example: We assume the label says something like:\n",
        "context_text = warnings_text\n",
        "\n",
        "# We'll pretend the user manually identified the answer to the question below:\n",
        "question_text = \"What are the warnings for this medication?\"\n",
        "answer_text = warnings_text  # For demonstration, we use the entire text as answer\n",
        "\n",
        "start_idx = context_text.find(answer_text)\n",
        "if start_idx == -1:\n",
        "    # If the text doesn't contain the answer due to formatting, just set it to 0\n",
        "    start_idx = 0\n",
        "end_idx = start_idx + len(answer_text)\n",
        "\n",
        "train_data = {\n",
        "    \"context\": [context_text],\n",
        "    \"question\": [question_text],\n",
        "    \"answers\": [{\"text\": [answer_text], \"answer_start\": [start_idx]}]\n",
        "}\n",
        "\n",
        "# Create a Hugging Face Dataset from this dictionary\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "\n",
        "# For a real scenario, also create a validation set (val_dataset)\n",
        "val_dataset = Dataset.from_dict(train_data)  # Just reusing the same for demo\n"
      ],
      "metadata": {
        "id": "oU9s05J3m_b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load a Pretrained QA Model and Tokenizer\n",
        "Let’s use the DistilBERT model fine-tuned on SQuAD (a common QA dataset). Then we’ll further fine-tune it on our custom data."
      ],
      "metadata": {
        "id": "M4e9OzsjpKcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "9b8Mdxp1nLam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preprocess the Data (Tokenization)\n",
        "We need a function that takes each data row (context + question + answers) and tokenizes it into the correct format for QA tasks."
      ],
      "metadata": {
        "id": "JOV-l4vVpTOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,  # typical max sequence length for QA\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # To train QA, we also need labels (start/end positions) in tokenized form\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, text in enumerate(examples[\"context\"]):\n",
        "        answer_starts = examples[\"answers\"][i][\"answer_start\"]\n",
        "        answer_texts = examples[\"answers\"][i][\"text\"]\n",
        "\n",
        "        # We assume a single answer. If multiple, you'd handle each accordingly.\n",
        "        start_char = answer_starts[0]\n",
        "        end_char = start_char + len(answer_texts[0])\n",
        "\n",
        "        # Convert character positions to token positions\n",
        "        token_start = inputs.char_to_token(i, start_char)\n",
        "        token_end = inputs.char_to_token(i, end_char - 1)\n",
        "\n",
        "        # Handle edge cases where char_to_token could return None\n",
        "        if token_start is None:\n",
        "            token_start = 0\n",
        "        if token_end is None:\n",
        "            token_end = len(inputs[\"input_ids\"][i]) - 1\n",
        "\n",
        "        start_positions.append(token_start)\n",
        "        end_positions.append(token_end)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "\n",
        "    return inputs\n"
      ],
      "metadata": {
        "id": "QTyoe71LnR7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now apply this to our datasets:**"
      ],
      "metadata": {
        "id": "BFLb9LI6phdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# For training, we typically remove columns we don't need\n",
        "train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"]\n",
        ")\n",
        "val_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "RA_KWQy4ndq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train the Model (Using Hugging Face Trainer)\n",
        "We’ll use the Trainer API, which handles a lot of the boilerplate for us."
      ],
      "metadata": {
        "id": "UobZTjiupwt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",      # Use eval_strategy instead of evaluation_strategy\n",
        "    logging_strategy=\"steps\",   # Log every N steps\n",
        "    logging_steps=1,            # Choose your logging interval\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "wgoP0RXGnkeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "NDh__zeQnqBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "# Launch 5 simulated experiments\n",
        "total_runs = 5\n",
        "for run in range(total_runs):\n",
        "  # 1️. Start a new run to track this script\n",
        "  wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"basic-intro\",\n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"experiment_{run}\",\n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"learning_rate\": 0.02,\n",
        "      \"architecture\": \"CNN\",\n",
        "      \"dataset\": \"CIFAR-100\",\n",
        "      \"epochs\": 10,\n",
        "      })\n",
        "\n",
        "  # This simple block simulates a training loop logging metrics\n",
        "  epochs = 10\n",
        "  offset = random.random() / 5\n",
        "  for epoch in range(2, epochs):\n",
        "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "\n",
        "      # 2️. Log metrics from your script to W&B\n",
        "      wandb.log({\"acc\": acc, \"loss\": loss})\n",
        "\n",
        "  # Mark the run as finished\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "QuY3QR5HwCmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OF73S0g390Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug Process- To Be Continued"
      ],
      "metadata": {
        "id": "rdlL1pbVdklp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets\n"
      ],
      "metadata": {
        "id": "jQuORGvmTU8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(qa_dataset))\n",
        "print(qa_dataset)\n",
        "print(qa_dataset.column_names)\n",
        "print(qa_dataset.num_rows)\n"
      ],
      "metadata": {
        "id": "JzS6SUbrYMuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset = qa_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"context\", \"question\", \"answers\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "-ny-WhCzW0it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(qa_dataset))\n"
      ],
      "metadata": {
        "id": "ujdiVwH7XH1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset = Dataset.from_dict(data_dict)\n",
        "qa_dataset = qa_dataset.train_test_split(test_size=0.2)\n"
      ],
      "metadata": {
        "id": "fJd2q427YA6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://api.fda.gov/drug/label.json\"\n",
        "params = {\"search\": \"brand_name:Keytruda\", \"limit\": 3}\n",
        "response = requests.get(base_url, params=params)\n",
        "data = response.json()\n",
        "results = data.get(\"results\", [])\n",
        "clinical_texts = []\n",
        "\n",
        "for record in results:\n",
        "    section_14 = record.get(\"clinical_studies\", [\"\"])[0]\n",
        "    clinical_texts.append(section_14 if section_14 else \"No data found\")\n"
      ],
      "metadata": {
        "id": "qD89lGwrY_rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(results))\n",
        "if not results:\n",
        "    print(\"No data found from FDA API!\")\n",
        "else:\n",
        "    print(\"Sample record keys:\", results[0].keys())\n"
      ],
      "metadata": {
        "id": "-LNnZHMMaz5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of clinical_texts:\", len(clinical_texts))\n",
        "if len(clinical_texts) > 0:\n",
        "    print(\"First item in clinical_texts:\", clinical_texts[0][:300])\n"
      ],
      "metadata": {
        "id": "UUiLy49Ka36Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts, questions, answers = [], [], []\n",
        "\n",
        "for text in clinical_texts:\n",
        "    contexts.append(text)\n",
        "    questions.append(\"What key clinical trials ...?\")\n",
        "    answers.append({\n",
        "        \"text\": [text],\n",
        "        \"answer_start\": [0]\n",
        "    })\n"
      ],
      "metadata": {
        "id": "TEsrYpvea_FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"contexts:\", contexts)\n",
        "print(\"questions:\", questions)\n",
        "print(\"answers:\", answers)\n"
      ],
      "metadata": {
        "id": "V2LFCm-ZbGnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(contexts) == 0:\n",
        "    print(\"No data to create a dataset!\")\n",
        "else:\n",
        "    data_dict = {\n",
        "        \"context\": contexts,\n",
        "        \"question\": questions,\n",
        "        \"answers\": answers\n",
        "    }\n",
        "    qa_dataset = Dataset.from_dict(data_dict)\n",
        "    print(qa_dataset)\n"
      ],
      "metadata": {
        "id": "7IF4H51nbJ8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    \"context\": [\n",
        "        \"Section 14: Some clinical data about Keytruda in KEYNOTE-006...\"\n",
        "    ],\n",
        "    \"question\": [\n",
        "        \"What key clinical trials are described in Section 14?\"\n",
        "    ],\n",
        "    \"answers\": [\n",
        "        {\n",
        "            \"text\": [\"Some clinical data about Keytruda in KEYNOTE-006...\"],\n",
        "            \"answer_start\": [13]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "qa_dataset = Dataset.from_dict(data_dict)\n",
        "print(\"Columns:\", qa_dataset.column_names)\n",
        "print(\"Num rows:\", qa_dataset.num_rows)\n",
        "print(qa_dataset[0])\n"
      ],
      "metadata": {
        "id": "8_rcznuabT_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "  \"search\": \"brand_name:KEYTRUDA\",\n",
        "  \"limit\": 3\n",
        "}\n"
      ],
      "metadata": {
        "id": "dT9TQtbRbYHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"API Raw JSON:\", data)\n"
      ],
      "metadata": {
        "id": "IGDxzMDBbf-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"search\": \"brand_name:KEYTRUDA\",\n",
        "    \"limit\": 3\n",
        "}\n",
        "url = \"https://api.fda.gov/drug/label.json\"\n",
        "response = requests.get(url, params=params)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "Palf7RoXbi5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    \"context\": [\"Section 14: The efficacy of KEYTRUDA in KEYNOTE-006...\"],\n",
        "    \"question\": [\"What key clinical trials are described in Section 14?\"],\n",
        "    \"answers\": [\n",
        "        {\n",
        "            \"text\": [\"The efficacy of KEYTRUDA in KEYNOTE-006...\"],\n",
        "            \"answer_start\": [13]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "qa_dataset = Dataset.from_dict(data_dict)\n",
        "print(qa_dataset)\n"
      ],
      "metadata": {
        "id": "08T8dKOYbpCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: openfda.brand_name\n",
        "params = {\n",
        "    \"search\": \"openfda.brand_name:KEYTRUDA\",\n",
        "    \"limit\": 5\n",
        "}\n",
        "\n",
        "# Option B: generic_name or substance_name\n",
        "params = {\n",
        "    \"search\": \"openfda.generic_name:pembrolizumab\",\n",
        "    \"limit\": 5\n",
        "}\n",
        "\n",
        "# Option C: broad search using brand name or partial\n",
        "params = {\n",
        "    \"search\": \"KEYTRUDA\",  # or brand_name:KEY* etc.\n",
        "    \"limit\": 5\n",
        "}\n"
      ],
      "metadata": {
        "id": "YIGQYM3sbvqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(base_url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "print(data)  # Inspect full output\n",
        "if \"results\" in data:\n",
        "    print(\"Found\", len(data[\"results\"]), \"results.\")\n",
        "else:\n",
        "    print(\"No results:\", data)\n"
      ],
      "metadata": {
        "id": "dsv5m14Fb_AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"results\" in data:\n",
        "    print(\"Sample keys in first record:\", data[\"results\"][0].keys())\n",
        "    # Check if there's a 'clinical_studies' or 'description', etc.\n"
      ],
      "metadata": {
        "id": "Lcoi6i6DcCnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    \"context\": [\n",
        "        \"Section 14: The efficacy of KEYTRUDA (pembrolizumab) was investigated in KEYNOTE-006...\"\n",
        "    ],\n",
        "    \"question\": [\n",
        "        \"What key clinical trials are described in Section 14?\"\n",
        "    ],\n",
        "    \"answers\": [\n",
        "        {\n",
        "            \"text\": [\"The efficacy of KEYTRUDA (pembrolizumab) was investigated in KEYNOTE-006...\"],\n",
        "            \"answer_start\": [13]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "qa_dataset = Dataset.from_dict(data_dict)\n",
        "print(qa_dataset)\n",
        "# Then map -> set_format -> train as you intended.\n"
      ],
      "metadata": {
        "id": "ok5Ay-OjcLp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "klagxN8ncTAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}